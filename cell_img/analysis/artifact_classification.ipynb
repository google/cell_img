{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsEYNxcycRRj"
      },
      "source": [
        "# Overview\n",
        "\n",
        "The object detection code identifies potential parasites and segments candidate patches into staining artifacts, hypnozoites, and schizonts. One concern: the object detector is trained on a limited set of human labeled examples. Drug candidates may affect parasite appearance in ways that lead to incorrect classification by the object detector. We are particularly concerned about damaged hypnozoites being mislabeled as staining artifacts.\n",
        "\n",
        "In this Colab we do two things:\n",
        "\n",
        "1) We build a simple model of parasite counts to get a rough estimate of the hypnozoite / artifact misclassification rate, then\n",
        "\n",
        "2) We built a better hypnozoite / artifact classifier and use these estimates to calibrate it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7XoXS5tl04j"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell only the FIRST time you connect to the colab kernel\n",
        "!pip install gcsfs\n",
        "!git clone https://github.com/google/cell_img\n",
        "!pip install --quiet -e cell_img\n",
        "!pip install lightgbm\n",
        "!pip install statsmodels --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWwXw0IGWzRj"
      },
      "outputs": [],
      "source": [
        "#@title For Cloud VM kernel, run this after restarting before granting access\n",
        "\n",
        "!ls /content/.config/\n",
        "!rm /content/.config/gce\n",
        "!rm /var/colab/mp\n",
        "\n",
        "import os\n",
        "try:\n",
        "  os.environ['NO_GCE_CHECK']\n",
        "  del os.environ['NO_GCE_CHECK']\n",
        "except KeyError:\n",
        "  pass\n",
        "try:\n",
        "  os.environ['GCE_METADATA_TIMEOUT']\n",
        "  del os.environ['GCE_METADATA_TIMEOUT']\n",
        "except KeyError:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1nf48-InGoE"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell after restarting your kernel. It will pop up window to grant access.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpsT_12Mqh_v"
      },
      "outputs": [],
      "source": [
        "CLOUD_BUCKET = 'bucket_name'\n",
        "\n",
        "# path to plate layout metadata\n",
        "METADATA_PATH = 'path/to/metadata'\n",
        "METADATA_PREFIX = f'gs://{CLOUD_BUCKET}/{METADATA_PATH}/metadata_prefix'\n",
        "\n",
        "# path to plate quality control annotations\n",
        "PLATE_ANNOTATION_FILE = f'gs://{CLOUD_BUCKET}/path/to/annotations.csv'\n",
        "\n",
        "# template for csv files with counts\n",
        "COUNT_CSV_TEMPLATE = f'gs://{CLOUD_BUCKET}/path/to/counts.csv'\n",
        "\n",
        "# path on the bucket where output should be saved\n",
        "OUTPUT_PATH = '/path/to/outputs'\n",
        "\n",
        "# path to a parquet file containing patch embeddings\n",
        "PATCH_PARQUET_TEMPLATE='path/to/patches.parquet'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrHNVt9YM5j0"
      },
      "outputs": [],
      "source": [
        "from typing import NamedTuple\n",
        "\n",
        "import datetime\n",
        "import fsspec\n",
        "import gc\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "from cell_img.analysis import optim_lib\n",
        "import gcsfs\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import lightgbm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from google.cloud import storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tYla2IoqMT4"
      },
      "outputs": [],
      "source": [
        "# Utility methods for reading Cloud files\n",
        "\n",
        "def glob_cloud(bucket, prefix, must_contain_str=None):\n",
        "  \"\"\"Returned files start with gs://[bucket]/[prefix] and contain the must_contain_str if provided.\"\"\"\n",
        "  client = storage.Client()\n",
        "  files_in_dir = [blob.name for blob in client.list_blobs(bucket, prefix=prefix)]\n",
        "  if not must_contain_str:\n",
        "    return [os.path.join('gs://%s' % bucket, f) for f in files_in_dir]\n",
        "\n",
        "  ret_val = []\n",
        "  for f in files_in_dir:\n",
        "    if must_contain_str in f:\n",
        "      ret_val.append(os.path.join('gs://%s' % bucket, f))\n",
        "  return ret_val\n",
        "\n",
        "\n",
        "def format_plate_strings(plate_names):\n",
        "  \"\"\"Format the plate strings as strings of five digit ints.\n",
        "\n",
        "  Args:\n",
        "    plate_names: A pd.Series of strings representing the plate names that we\n",
        "      want to format.\n",
        "  Raises:\n",
        "    ValueError: If plate_names contains a name that is more than five digits\n",
        "      long.\n",
        "  Returns:\n",
        "    formatted_plates: A pd.Series representing the formatted plate names.\n",
        "  \"\"\"\n",
        "  # Format the plate strings as 5 character strings with no decimal\n",
        "  formatted_plates = plate_names.astype(str).apply(\n",
        "      lambda x: x.split('.')[0].zfill(5))\n",
        "  # If any of the plates are more than 5 digits, scream loudly.\n",
        "  len_plate_names = np.array([len(p) for p in formatted_plates.values])\n",
        "  if np.any(len_plate_names \u003e 5):\n",
        "    raise ValueError('Plate name \u003e 5 characters found')\n",
        "  # If any of the plates have non-digit characters, scream loudly.\n",
        "  if not np.all([re.match(r'^\\d+$', p) for p in formatted_plates.values]):\n",
        "    raise ValueError('Plate with non-digit characters found')\n",
        "  return formatted_plates\n",
        "\n",
        "\n",
        "def compress_df(df: pd.DataFrame) -\u003e pd.DataFrame:\n",
        "  \"\"\"Reduce the byte size of columns.\"\"\"\n",
        "  for col in df.columns:\n",
        "    if df[col].dtype == np.float64:\n",
        "      df[col] = df[col].astype(np.float32)\n",
        "    elif df[col].dtype == np.int64:\n",
        "      df[col] = df[col].astype(np.int32)\n",
        "  return df\n",
        "\n",
        "\n",
        "def create_parquet_filter(field, values_to_keep):\n",
        "  # Create the filter for all plates\n",
        "  return [(field, 'in', set(values_to_keep))]\n",
        "\n",
        "\n",
        "def load_parquet_to_emb_df(bucket, prefix, batch_list, filter_list=None):\n",
        "  # Read the new Parquet output\n",
        "  emb_df_list = []\n",
        "  filepaths = []\n",
        "  for b in batch_list:\n",
        "    this_b = glob_cloud(bucket, prefix % b)\n",
        "    filepaths.extend(this_b)\n",
        "  print('There are a total of %d shards' % len(filepaths))\n",
        "\n",
        "  t0 = time.time()\n",
        "  for i, shard_path in enumerate(filepaths):\n",
        "    if (i + 1) % 100 == 0:\n",
        "      print(f'shard {i + 1}, {time.time() - t0:.1f} sec')\n",
        "    with fsspec.open(shard_path) as f:\n",
        "      if filter_list:\n",
        "        shard_df = pd.read_parquet(\n",
        "            f,\n",
        "            #columns=['col_name'],  # only read these columns\n",
        "            filters=filter_list,\n",
        "          )\n",
        "      else:\n",
        "        shard_df = pd.read_parquet(f)\n",
        "    shard_df = compress_df(shard_df)\n",
        "    emb_df_list.append(shard_df)\n",
        "    gc.collect()\n",
        "\n",
        "  emb_df = pd.concat(emb_df_list)\n",
        "  if len(emb_df) == 0:\n",
        "    raise ValueError('The embedding dataframe is empty, did you use the right filters?')\n",
        "\n",
        "  for col in ['image', 'channel_order']:\n",
        "    if col in emb_df.columns:\n",
        "      emb_df.drop(columns=[col], inplace=True)\n",
        "\n",
        "  # expand the embedding\n",
        "  tmp_df = pd.DataFrame([pd.Series(x) for x in emb_df.embedding])\n",
        "  tmp_df.columns = [str(x) for x in range(192)]\n",
        "  emb_df = pd.concat([emb_df.reset_index(), tmp_df], axis=1)\n",
        "  emb_df.drop(columns=['embedding'], inplace=True)\n",
        "\n",
        "  # expand the parasite stage inference values\n",
        "  if 'parasite_stage_infer' in emb_df.columns:\n",
        "    tmp_df = pd.DataFrame([pd.Series(x) for x in emb_df.parasite_stage_infer])\n",
        "    tmp_df.columns = ['stage_infer_artifact', 'stage_infer_hypnozoite', 'stage_infer_schizont']\n",
        "    emb_df = pd.concat([emb_df, tmp_df], axis=1)\n",
        "    emb_df.drop(columns=['parasite_stage_infer', 'parasite_stage_names'], inplace=True)\n",
        "\n",
        "  # set the index\n",
        "  emb_df.set_index([c for c in emb_df.columns if not c in [str(x) for x in range(192)]], inplace=True)\n",
        "\n",
        "  return emb_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gcpwCdHvT8M"
      },
      "source": [
        "## Load and preprocess the parasite counts\n",
        "\n",
        "The code below is specific to our experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV5n_D-9qp1E"
      },
      "outputs": [],
      "source": [
        "# get screen plates that have passed QC as well as dose response plates\n",
        "plate_ann_df = pd.read_csv(PLATE_ANNOTATION_FILE)\n",
        "plate_ann_df['plate'] = format_plate_strings(plate_ann_df['plate'])\n",
        "\n",
        "screen_plates = set(plate_ann_df.query('singlePointScreening == \"yes\"').plate)\n",
        "qc_screen_plates = set(plate_ann_df.query('singlePointScreening == \"yes\" and RZprime \u003e 0').plate)\n",
        "qc_screen_batches = set(plate_ann_df.query('singlePointScreening == \"yes\" and RZprime \u003e 0').experiment)\n",
        "\n",
        "del plate_ann_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t884UPjmtSzy"
      },
      "outputs": [],
      "source": [
        "# load a dataframe of parasite and artifact counts for all plates\n",
        "count_df = []\n",
        "for batch in qc_screen_batches:\n",
        "  batch_count_df = pd.read_csv(COUNT_CSV_TEMPLATE % batch)\n",
        "  batch_count_df['plate'] = format_plate_strings(batch_count_df['plate'])\n",
        "  batch_count_df = batch_count_df[batch_count_df['plate'].isin(qc_screen_plates)].copy()\n",
        "  count_df.append(batch_count_df)\n",
        "count_df = pd.concat(count_df, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXue4iPa2uXN"
      },
      "outputs": [],
      "source": [
        "count_df = count_df.rename(columns={'num_artifact': 'ml_artifact', 'num_hypnozoite': 'ml_hypnozoite', 'num_schizont': 'ml_schizont'})\n",
        "count_df = count_df.drop(columns=['num_obj'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvaczKS_vzel"
      },
      "outputs": [],
      "source": [
        "count_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4A7Ylesv2Ek"
      },
      "outputs": [],
      "source": [
        "count_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Fg3qOIywnkt"
      },
      "outputs": [],
      "source": [
        "# Find metadata paths\n",
        "metadata_paths={batch:[] for batch in qc_screen_batches}\n",
        "glob_list = glob_cloud(CLOUD_BUCKET, METADATA_PATH)\n",
        "for blob_name in glob_list:\n",
        "  for batch in qc_screen_batches:\n",
        "    if blob_name.startswith(f'{METADATA_PREFIX}{batch}'):\n",
        "      metadata_paths[batch].append(blob_name)\n",
        "\n",
        "missing_batches = []\n",
        "for batch, paths in metadata_paths.items():\n",
        "  if len(paths) == 0:\n",
        "    missing_batches.append(batch)\n",
        "\n",
        "if missing_batches:\n",
        "  raise Exception('Unable to find metadata for batches: %s' % missing_batches)\n",
        "\n",
        "# Only keep the paths for the latest metadata\n",
        "metadata_paths = {key:sorted(val)[-1] for key, val in metadata_paths.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXbdb6u3yiLZ"
      },
      "outputs": [],
      "source": [
        "# Load metadata from cloud\n",
        "metadata_list = []\n",
        "for met in metadata_paths.values():\n",
        "  metadata_list.append(pd.read_csv(met))\n",
        "metadata = pd.concat(metadata_list)\n",
        "metadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA3pD8N2ysCv"
      },
      "outputs": [],
      "source": [
        "# These are the columns in our metadata to keep and how to rename them.\n",
        "METADATA_COLS = ['plate', 'Metadata_Well', 'actives', 'concentration', 'hepLot',\n",
        "                 'blindedConcept', 'flag-IQCH30', 'flag-Nuclei',\n",
        "                 'Count_Nuclei', 'hypnozoite', 'schizont']\n",
        "METADATA_RENAME = {\n",
        "    'Metadata_Well': 'well',\n",
        "    'hepLot': 'hep_lot',\n",
        "    'blindedConcept': 'blinded_concept',\n",
        "    'flag-IQCH30': 'flag_iqch30',\n",
        "    'flag-Nuclei': 'flag_nuclei',\n",
        "    'Count_Nuclei': 'cp_hepatocyte',\n",
        "    'hypnozoite': 'cp_hypnozoite',\n",
        "    'schizont': 'cp_schizont'}\n",
        "\n",
        "metadata = metadata[METADATA_COLS].rename(columns=METADATA_RENAME)\n",
        "metadata['actives'] = metadata.actives.apply(\n",
        "    lambda a: a.replace(' ', '_')\n",
        ")\n",
        "metadata['plate'] = metadata.plate.apply(lambda x: str(int(x)).zfill(5))\n",
        "metadata = metadata[metadata['plate'].isin(qc_screen_plates)].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OjROeI6zqGi"
      },
      "outputs": [],
      "source": [
        "metadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-dN8YW10L9k"
      },
      "outputs": [],
      "source": [
        "# Join counts and metadata and clean up column names, fix NaNs, cast values to floats\n",
        "tmp_df = count_df.merge(metadata, on=['plate', 'well'])\n",
        "\n",
        "STAGES = ['artifact', 'hypnozoite', 'schizont', 'hepatocyte']\n",
        "for s in STAGES:\n",
        "  for colname in ['ml_%s' % s, 'cp_%s' % s]:\n",
        "    if colname not in tmp_df.columns:\n",
        "      continue\n",
        "    values = tmp_df[colname].to_numpy().astype(np.float32)\n",
        "    values[~np.isfinite(values)] = 0.\n",
        "    tmp_df[colname] = values\n",
        "\n",
        "for col in ['concentration']:\n",
        "  tmp_df[col] = tmp_df[col].to_numpy().astype(np.float32)\n",
        "\n",
        "for col in ['flag_iqch30', 'flag_nuclei']:\n",
        "  tmp_df[col] = (tmp_df[col].to_numpy() == 'yes')\n",
        "\n",
        "count_df = tmp_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywlXV0Xa0Pi8"
      },
      "outputs": [],
      "source": [
        "# force infected control concentration to be uniform\n",
        "# (workaround for some possibly bad metadata in a few plates)\n",
        "corrected_df = []\n",
        "for _, plate_df in count_df.groupby(['plate']):\n",
        "  plate_df = plate_df.copy()\n",
        "  is_infected_control = plate_df.actives == 'infected_control'\n",
        "  if np.sum(is_infected_control):\n",
        "    conc = plate_df.concentration.to_numpy()\n",
        "    conc[is_infected_control] = np.median(conc[is_infected_control])\n",
        "    plate_df['concentration'] = conc\n",
        "  corrected_df.append(plate_df)\n",
        "count_df = pd.concat(corrected_df)\n",
        "del corrected_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFdl9VYC0WO0"
      },
      "outputs": [],
      "source": [
        "count_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC1d83vT0UAP"
      },
      "outputs": [],
      "source": [
        "# replace ml_parasite, which currently counts all patches, with a count of all parasites\n",
        "count_df['ml_parasite'] = count_df['ml_hypnozoite'] + count_df['ml_schizont']\n",
        "count_df['cp_parasite'] = count_df['cp_hypnozoite'] + count_df['cp_schizont']\n",
        "\n",
        "# get median parasite and hepatocyte counts for infected controls\n",
        "COUNT_COLS = ['ml_parasite', 'ml_hypnozoite',\n",
        "              'cp_parasite', 'cp_hypnozoite', 'cp_hepatocyte']\n",
        "count_df_med = (count_df[count_df.actives == 'infected_control'].groupby(['plate'])\n",
        "    [COUNT_COLS].median().rename(columns={col:col + '_med_neg' for col in COUNT_COLS}).reset_index())\n",
        "# add in infected control medians so we can compute inhibition\n",
        "count_df = count_df.reset_index().merge(count_df_med, on='plate')\n",
        "\n",
        "count_df_med = (count_df[count_df.actives == 'active_control'].groupby(['plate'])\n",
        "    [COUNT_COLS].median().rename(columns={col:col + '_med_pos' for col in COUNT_COLS}).reset_index())\n",
        "# add in active control medians so we can adjust inhibition\n",
        "count_df = count_df.reset_index().merge(count_df_med, on='plate')\n",
        "\n",
        "for method in ['ml', 'cp']:\n",
        "  count_df[f'inhibition_{method}_par'] = (1. - np.clip(count_df[f'{method}_parasite'] / count_df[f'{method}_parasite_med_neg'], 0, 1)).astype(np.float32)\n",
        "  count_df[f'inhibition_{method}_hyp'] = (1. - np.clip(count_df[f'{method}_hypnozoite'] / count_df[f'{method}_hypnozoite_med_neg'], 0, 1)).astype(np.float32)\n",
        "count_df[f'inhibition_cp_hyp_act'] = (1. - np.clip((count_df[f'cp_hypnozoite'] - count_df[f'cp_hypnozoite_med_pos'])/\n",
        "                                                   (count_df[f'cp_hypnozoite_med_neg'] - count_df[f'cp_hypnozoite_med_pos']), 0, 1)).astype(np.float32)\n",
        "count_df[f'inhibition_cp_par_act'] = (1. - np.clip((count_df[f'cp_parasite'] - count_df[f'cp_parasite_med_pos'])/\n",
        "                                                     (count_df[f'cp_parasite_med_neg'] - count_df[f'cp_parasite_med_pos']), 0, 1)).astype(np.float32)\n",
        "\n",
        "if 'level_0' in count_df.columns:\n",
        "  count_df = count_df.drop(columns=['level_0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbJwIRsx-z12"
      },
      "outputs": [],
      "source": [
        "count_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snXU29LfAQsF"
      },
      "outputs": [],
      "source": [
        "count_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il1l9MMe-xVY"
      },
      "outputs": [],
      "source": [
        "# Set of plates / wells to use:\n",
        "# * Plate must pass QC\n",
        "# * Median hypnozoites for infected controls must be \u003e= 10\n",
        "# * Drop wells with flag_IQCH30 set (the flag indicates a bad well)\n",
        "# * Restrict to control wells\n",
        "\n",
        "subset = ((count_df.plate.isin(qc_screen_plates)) \u0026\n",
        "          (count_df.ml_hypnozoite_med_neg \u003e= 10) \u0026\n",
        "          (~count_df.flag_iqch30) \u0026\n",
        "          (count_df.actives.isin({'infected_control', 'uninfected_control', 'active_control'})))\n",
        "\n",
        "control_df = count_df[subset].copy().sort_values(['plate', 'well']).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6bfcArLAKq_"
      },
      "outputs": [],
      "source": [
        "control_df['ml_hyp_or_art'] = control_df['ml_hypnozoite'] + control_df['ml_artifact']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW1EF-H-Al7l"
      },
      "outputs": [],
      "source": [
        "# Drop wells with unusual numbers of hypnozoites + artifacts\n",
        "#\n",
        "# Here we'll use Tukey's fence (Q3 + 3 * IQR) to flag outliers on each plate\n",
        "# See https://en.wikipedia.org/wiki/Outlier#Tukey's_fences\n",
        "control_df_filtered = []\n",
        "for _, df_plate in control_df.groupby('plate'):\n",
        "  for method in ['ml']:\n",
        "    col = f'{method}_hyp_or_art'\n",
        "    q1, q3 = np.quantile(control_df[col], (0.25, 0.75))\n",
        "    iqr = q3 - q1\n",
        "    threshold = q3 + 3. * iqr  # Tukey's fence for outliers\n",
        "    df_plate = df_plate[df_plate[col] \u003c threshold]\n",
        "  bad_plate = False\n",
        "  for actives in ['infected_control', 'active_control', 'uninfected_control']:\n",
        "    if np.sum(df_plate.actives == actives) == 0:\n",
        "      bad_plate = True\n",
        "  if not bad_plate:\n",
        "    control_df_filtered.append(df_plate)\n",
        "control_df = pd.concat(control_df_filtered)\n",
        "del control_df_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWn6YG2lYjPS"
      },
      "source": [
        "## Diagnostics\n",
        "\n",
        "A couple of plots that show something is amiss:\n",
        "\n",
        "1) First, we see that the object detector consistently finds more staining artifacts in infected control wells than in uninfected control wells. That suggests that we may be mislabeling some hypnozoites as staining artifacts.\n",
        "\n",
        "2) Second, we see that the more hypnozoites we have in the infected control wells, the greater the gap in artifacts. Again, this suggests the object detector may be mislabling a fraction of hypnozoites."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6gWThkzPDu4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "art_uninf = control_df[control_df.actives == 'uninfected_control'].groupby('plate')['ml_artifact'].mean()\n",
        "art_inf = control_df[control_df.actives == 'infected_control'].groupby('plate')['ml_artifact'].mean()\n",
        "\n",
        "_, bins, _ = plt.hist(art_uninf, label='Uninfected controls', alpha=0.5)\n",
        "plt.hist(art_inf, label='Infected controls', bins=bins, alpha=0.5)\n",
        "plt.title('Mean artifact counts')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "hyp = control_df[control_df.actives == 'infected_control'].groupby('plate')['ml_hypnozoite'].mean()\n",
        "\n",
        "x = hyp\n",
        "y = (art_inf - art_uninf)\n",
        "vmax = max(np.max(x), np.max(y))\n",
        "sns.regplot(x=x, y=y)\n",
        "plt.title('Excess infected control artifacts as a function of hypnozoite counts')\n",
        "plt.xlabel('Infected control hypnozoites')\n",
        "plt.ylabel('Infected - uninfected artifacts')\n",
        "plt.hlines(0, 0, vmax, ls='--', color='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qxEUu8gxKz9"
      },
      "source": [
        "## Estimating the true number of artifacts and hypnozoites\n",
        "\n",
        "We're going to use a simple model to estimate the true numbers of hypnozoites and artifacts.\n",
        "\n",
        "Our model makes a few core assumptions:\n",
        "\n",
        "1) Within each plate, the number of artifacts in a well is roughly consistent. We model the distribution of artifacts using a negative binomial distribution.\n",
        "\n",
        "2) Within each plate, the number of hypnozoites in the infected control wells are also negative binomially distributed as are the number of hypnozoites in the active control wells, and the two sets of wells share an overdispersion coefficient.\n",
        "\n",
        "We can use these assumptions to compute plate-specific maximum likelihood estimates for the mean number of artifacts and hypnozoites in control wells.\n",
        "\n",
        "We observe that the model estimates higher hypnozoite counts than does the object detector, which suggests that the object detector may be systematically misclassifying a subset of hypnozoites as staining artifacts.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDOOA4p8OEKr"
      },
      "source": [
        "### Model assumptions in detail\n",
        "\n",
        "1) We assume the number of artifacts we observe in a well has a negative binomial distribution with mean given by\n",
        "\n",
        "$$\\text{mean}_{\\text{a, plate, treatment}} = a_{\\text{plate}}$$\n",
        "\n",
        "and variance\n",
        "\n",
        "$$\\text{var}_{\\text{a, plate, treatment}} = \\theta_{a, \\text{plate}} a_{\\text{plate}}$$\n",
        "\n",
        "where $\\theta_{a, \\text{plate}} \\geq 1$ represents a plate-specific level of overdispersion.\n",
        "\n",
        "2) We assume the number of hypnozoites is also negative binomially distributed with mean\n",
        "\n",
        "$$\\text{mean}_{\\text{h, plate, treatment}} = h_{\\text{plate}} (1 - \\eta_{\\text{treatment}})$$\n",
        "\n",
        "and variance\n",
        "\n",
        "$$\\text{var}_{\\text{h, plate, treatment}} = \\theta_{h, \\text{plate}} (1 - \\eta_{\\text{treatment}}) h_{\\text{plate}}$$\n",
        "\n",
        "where again $\\theta_{h, \\text{plate}} \\geq 1$ represents a plate-specific level of overdispersion.\n",
        "\n",
        "Here $\\eta_{\\text{treatment}}$ is the parasite inhibition for a treatment, i.e. the fraction of parasites that die after a treatment.\n",
        "\n",
        "We assume the following:\n",
        "* For the infected control wells, $\\eta_{\\text{infected}} = 0$.\n",
        "* For the uninfected control wells, $\\eta_{\\text{uninfected}} = 1$.\n",
        "* For the active control wells, we allow the treatment strength to\n",
        "vary from plate to plate, i.e. $\\eta_{\\text{active}} = k_{\\text{plate}}$.\n",
        "\n",
        "\n",
        "3) The sum of two negative binomially-distributed random variables doesn't have a nice closed-form distribution except in the special case in which the variables have a common overdispersion. We'll approximte the sum with another negative binomial distribution whose mean and variance equal the mean and variance for the sum, i.e. a negative binomial distribution with mean\n",
        "\n",
        "$$a_{\\text{plate}} + (1 - \\eta_{\\text{treatment}}) h_{\\text{plate}}$$\n",
        "\n",
        "and variance\n",
        "\n",
        "$$\\theta_{a, \\text{plate}} a_{\\text{plate}} +\n",
        "\\theta_{h, \\text{plate}} (1 - \\eta_{\\text{treatment}}) h_{\\text{plate}}\n",
        "$$\n",
        "\n",
        "\n",
        "We will use gradient ascent to find parameter values that maximize the likelihood of the observed counts for control wells under the model assumptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qr0kSHnSykr"
      },
      "outputs": [],
      "source": [
        "sorted_plates = sorted(set(control_df.plate))\n",
        "plate_to_index = {p:i for i, p in enumerate(sorted_plates)}\n",
        "plate_index = jnp.array([plate_to_index[p] for p in control_df.plate])\n",
        "hyp_or_art_ml = jnp.array(control_df['ml_hyp_or_art'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS65l5h5tYcY"
      },
      "outputs": [],
      "source": [
        "# We're going to treat all sample wells as different treatments\n",
        "# treatment_index 0 = infected control\n",
        "# treatment_index 1 = uninfected control\n",
        "# treatment_index 2 = active control\n",
        "# treatment_index 3 and up = everything else\n",
        "\n",
        "max_treatment = 2\n",
        "treatment_index = []\n",
        "for actives in control_df.actives:\n",
        "  if actives == 'infected_control':\n",
        "    treatment_index.append(0)\n",
        "  elif actives == 'uninfected_control':\n",
        "    treatment_index.append(1)\n",
        "  elif actives == 'active_control':\n",
        "    treatment_index.append(2)\n",
        "  else:\n",
        "    raise ValueError(actives)\n",
        "treatment_index = jnp.array(treatment_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9_wJOlXZKQ4"
      },
      "outputs": [],
      "source": [
        "control_df['plate_index'] = plate_index\n",
        "control_df['treatment_index'] = treatment_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB16pzUTPUPI"
      },
      "outputs": [],
      "source": [
        "MAX_ETA = 0.999\n",
        "\n",
        "class Model(NamedTuple):\n",
        "  # our parameters are constrained;\n",
        "  # we'll enforce the constraints via invertible transforms and\n",
        "  # do gradient descent on unconstrained versions of the parameters\n",
        "  a_unconstrained: jnp.ndarray  # (n_plates,)\n",
        "  h_unconstrained: jnp.ndarray  # (n_plates,)\n",
        "  theta_a_unconstrained: jnp.ndarray  # (n_plates,)\n",
        "  theta_h_unconstrained: jnp.ndarray  # (n_plates,)\n",
        "  eta_active_unconstrained: jnp.ndarray  # (n_plates)\n",
        "\n",
        "  @property\n",
        "  def a(self):\n",
        "    return jnp.exp(self.a_unconstrained)\n",
        "\n",
        "  @property\n",
        "  def h(self):\n",
        "    return jnp.exp(self.h_unconstrained)\n",
        "\n",
        "  @property\n",
        "  def theta_a(self):\n",
        "    return 1 + jnp.exp(self.theta_a_unconstrained)\n",
        "\n",
        "  @property\n",
        "  def theta_h(self):\n",
        "    return 1 + jnp.exp(self.theta_h_unconstrained)\n",
        "\n",
        "  @property\n",
        "  def eta_active(self):\n",
        "    # active control effect - constrain to (0, MAX_ETA)\n",
        "    return MAX_ETA * jax.scipy.special.expit(self.eta_active_unconstrained)\n",
        "\n",
        "  def get_eta(self, treatment_index: jnp.ndarray, plate_index: jnp.ndarray) -\u003e jnp.ndarray:\n",
        "    return jnp.where(\n",
        "        treatment_index == 0,\n",
        "        0.,  # infected control\n",
        "        jnp.where(\n",
        "            treatment_index == 1,\n",
        "            1.,  # uninfected control\n",
        "            self.eta_active[plate_index],  # active control\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InpRcjSQTHyi"
      },
      "outputs": [],
      "source": [
        "n_plates = jnp.max(plate_index) + 1\n",
        "n_treatments = jnp.max(treatment_index) + 1\n",
        "model_init = Model(\n",
        "    a_unconstrained=jnp.zeros((n_plates,)),\n",
        "    h_unconstrained=jnp.zeros((n_plates,)),\n",
        "    theta_a_unconstrained=jnp.zeros((n_plates,)),\n",
        "    theta_h_unconstrained=jnp.zeros((n_plates,)),\n",
        "    eta_active_unconstrained=jnp.zeros((n_plates,)),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOa39-01SGEN"
      },
      "outputs": [],
      "source": [
        "def get_loss_fn(\n",
        "    hyp_or_art: jnp.ndarray,\n",
        "    plate_index: jnp.ndarray,\n",
        "    treatment_index: jnp.ndarray):\n",
        "  def loss(model: Model) -\u003e jnp.ndarray:\n",
        "    eta = model.get_eta(treatment_index=treatment_index, plate_index=plate_index)\n",
        "    mean_a = model.a[plate_index]\n",
        "    var_a = model.theta_a[plate_index] * mean_a\n",
        "    mean_h = model.h[plate_index] * (1 - eta)\n",
        "    var_h = model.theta_h[plate_index] * mean_h\n",
        "\n",
        "    mean_ah = mean_a + mean_h\n",
        "    var_ah = var_a + var_h\n",
        "\n",
        "    # for scipy.stats.nbinom\n",
        "    # n = number of successes\n",
        "    # p = P(success)\n",
        "    # k = number of failures\n",
        "    # mean(k) = n(1-p)/p\n",
        "    # var(k) = n(1-p)/p^2 = mean / p\n",
        "\n",
        "    p = mean_ah / var_ah\n",
        "    n = mean_ah * p / (1 - p)\n",
        "\n",
        "    log_prob = jax.scipy.stats.nbinom.logpmf(k=hyp_or_art, n=n, p=p)\n",
        "    return -jnp.sum(log_prob)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK461vDneHv7"
      },
      "outputs": [],
      "source": [
        "loss_fn_ml = jax.jit(get_loss_fn(\n",
        "    hyp_or_art=hyp_or_art_ml,\n",
        "    plate_index=plate_index,\n",
        "    treatment_index=treatment_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6Ae9DKGem8g"
      },
      "outputs": [],
      "source": [
        "print('initial loss', loss_fn_ml(model_init))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VtRs8vKfFOK"
      },
      "outputs": [],
      "source": [
        "model = optim_lib.adam_optimize(\n",
        "    loss_fn_ml,\n",
        "    model_init,\n",
        "    learning_rate=0.01,\n",
        "    train_steps=20000,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epqjULLBJY8K"
      },
      "source": [
        "A quick sanity check of the fitted parameters.\n",
        "\n",
        "Make sure there are no crazy outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_Gm9shqchOY"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "axes[0].hist(np.array(model.a), bins=50)\n",
        "axes[0].set_title('Mean artifacts')\n",
        "\n",
        "axes[1].hist(np.array(model.theta_a), bins=50)\n",
        "axes[1].set_title('Overdispersion scaling')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cnIxDIncuWc"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "axes[0].hist(np.array(model.h), bins=50)\n",
        "axes[0].set_title('Mean hypnozoites')\n",
        "\n",
        "axes[1].hist(np.array(model.theta_h), bins=50)\n",
        "axes[1].set_title('Overdispersion hypnozoites')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQuGV_leVnDY"
      },
      "source": [
        "### Modeling results\n",
        "\n",
        "First, we see that the estimated mean number of artifacts for each plate is well approximated by the median of the uninfected control artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGuQlrmg0dL4"
      },
      "outputs": [],
      "source": [
        "# compute rough estimates of the fraction of hypnozoites in each control well\n",
        "estimated_hyp_fraction = []\n",
        "\n",
        "for _, row in control_df.iterrows():\n",
        "  plate_index = row['plate_index']\n",
        "  treatment_index = row['treatment_index']\n",
        "  eta = model.get_eta(treatment_index=treatment_index, plate_index=plate_index)\n",
        "  mean_artifacts = model.a[plate_index]\n",
        "  mean_hypnozoites = model.h[plate_index] * (1 - eta)\n",
        "  estimated_hyp_fraction.append(mean_hypnozoites / (mean_hypnozoites + mean_artifacts))\n",
        "control_df['estimated_hyp_fraction'] = estimated_hyp_fraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rODegnuWmU68"
      },
      "outputs": [],
      "source": [
        "uninfected_mean = control_df[control_df.actives == 'uninfected_control'].groupby('plate').mean()\n",
        "infected_mean = control_df[control_df.actives == 'infected_control'].groupby('plate').mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0fid3vKM7YO"
      },
      "outputs": [],
      "source": [
        "# Compare mean uninfected control artifact counts to model estimated counts\n",
        "x = np.array(uninfected_mean['ml_artifact'])\n",
        "y = np.array(model.a)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.regplot(\n",
        "    x=x,\n",
        "    y=y,\n",
        "    marker='.')\n",
        "vmax = max(np.max(x), np.max(y))\n",
        "plt.plot((0, vmax), (0, vmax), linestyle='--', color='gray')\n",
        "plt.xlim(0, vmax)\n",
        "plt.ylim(0, vmax)\n",
        "\n",
        "plt.title('Model')\n",
        "plt.xlabel('Mean uninfected control artifacts')\n",
        "plt.ylabel('Model uninfected control artifacts')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCS3zZe8aDwe"
      },
      "source": [
        "Second, we see that the model suggests that the object detector is underestimating the number of true hypnozoites by about ~10%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAQxEgbhVtEP"
      },
      "outputs": [],
      "source": [
        "# Compare mean infected control hypnozoite counts to model estimated counts\n",
        "\n",
        "x = np.array(infected_mean['ml_hypnozoite'])\n",
        "y = np.array(model.h)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.regplot(\n",
        "    x=x,\n",
        "    y=y,\n",
        "    marker='.')\n",
        "plt.title('Model')\n",
        "plt.xlabel('Median infected control hypnozoites')\n",
        "plt.ylabel('Model infected control hypnozoites')\n",
        "vmax = max(np.max(x), np.max(y))\n",
        "plt.plot((0, vmax), (0, vmax), linestyle='--', color='gray')\n",
        "plt.xlim(0, vmax)\n",
        "plt.ylim(0, vmax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqqbJkq-agQV"
      },
      "outputs": [],
      "source": [
        "m = sm.OLS(y, x)\n",
        "results = m.fit()\n",
        "results.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFB78nmvPXPS"
      },
      "source": [
        "## A better artifact classifier\n",
        "\n",
        "The analysis above suggests that the object detector misclassifies some hypnozoites as staining artifacts. Below we build a better classifier.\n",
        "\n",
        "Our challenge is that our supply of labeled data is limited, and in some cases our expert labelers may be hard-pressed to determine whether an object is a hypnozoite or a staining artifact. Adding to the difficulty is the possibility that some drug candidates may make change the appearance of parasites in ways we can't anticipate.\n",
        "\n",
        "Rather than trying to create a better hand-labeled dataset to use for a better supervised classifier, we will take advantage of our experimental design. Each plate contains a set of uninfected control wells that contain no parasites. We can build a classifier that distinguishes between objects found in uninfected control wells and objects found in wells with parasites. For such a classifier, we have ground truth for all objects found by the object detector. If we are willing to assume that staining artifacts from infected and uninfected wells are indistinguishable, the only way such a classifier can differentiate between objects from infected and uninfected wells is by distinguishing parasites from staining artifacts. If we can identify an object as a parasite, we know that it must have come from an infected well. In contrast, under our assumptions, we will be unable to determine whether a staining artifact comes from an infected or uninfected well.\n",
        "\n",
        "The resulting infected well/uninfected well classifier is effectively a hypnozoite/artifact classifier with miscalibrated probabilities. Objects that the classifier indicates have a high probability of coming from an infected well are likely parasites, while objects for which the probability of coming from an infected or uninfected well are roughly equal are likely staining artifacts. To turn our infected / uninfected well classifier into a parasite / staining artifact classifier, we need to recalibrate the output probabilities.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2-FD9D-ur6R"
      },
      "outputs": [],
      "source": [
        "# loading takes ~20 minutes\n",
        "subset = ((count_df.plate.isin(qc_screen_plates)) \u0026\n",
        "          (count_df.ml_hypnozoite_med_neg \u003e= 10) \u0026\n",
        "          (~count_df.flag_iqch30))\n",
        "\n",
        "batches = sorted(set(count_df[subset]['batch']))\n",
        "plates = sorted(set(count_df[subset]['plate']))\n",
        "\n",
        "filter = create_parquet_filter('plate', plates)\n",
        "emb_df = load_parquet_to_emb_df(\n",
        "    bucket=CLOUD_BUCKET,\n",
        "    prefix=PATCH_PARQUET_TEMPLATE,\n",
        "    batch_list=batches,\n",
        "    filter_list=filter)\n",
        "\n",
        "emb_df_bak = emb_df.copy()  # make a backup copy so we don't have to reload from disk!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrO_Uh95tTNE"
      },
      "outputs": [],
      "source": [
        "emb_df = emb_df_bak.copy()  # recover from backup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0G0EYNEcx16"
      },
      "outputs": [],
      "source": [
        "emb_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff66MLzUwcl4"
      },
      "outputs": [],
      "source": [
        "emb_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC9H6Qd4ueHb"
      },
      "outputs": [],
      "source": [
        "emb_df = emb_df.reset_index().merge(\n",
        "    count_df[subset].reset_index()[['plate', 'well', 'actives']],\n",
        "    how='inner',\n",
        "    on=['plate', 'well']).drop(columns=['index']).copy()  # copy to reduce fragmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfGHM2XdwlnI"
      },
      "outputs": [],
      "source": [
        "emb_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzzPUwfCnBxh"
      },
      "source": [
        "### Classifying objects as coming from infected or uninfected wells\n",
        "\n",
        "We use object embeddings as features for classification. We will use LightGBM as our classifier because it is fast and robust to overfitting. We regularize by limiting tree depth. We train on 80% of the data and use 10% each for validation and test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-AerUqjtaBi"
      },
      "outputs": [],
      "source": [
        "# split plates into training / validation / test sets\n",
        "np.random.seed(123)\n",
        "\n",
        "DATA_FRACTION = 1.\n",
        "\n",
        "fraction_train = 0.8\n",
        "fraction_validation = 0.1\n",
        "fraction_test = 1. - fraction_train - fraction_validation\n",
        "\n",
        "plates = sorted(set(emb_df.plate))\n",
        "n_plates = len(plates)\n",
        "perm = np.random.permutation(n_plates)\n",
        "random_subset = (emb_df['actives'] == 'uninfected_control') | (np.random.rand(emb_df.shape[0]) \u003c= DATA_FRACTION)\n",
        "\n",
        "train_plates = {plates[i] for i in perm[:int(fraction_train * n_plates)]}\n",
        "validation_plates = {plates[i] for i in perm[int(fraction_train * n_plates):int((1. - fraction_test) * n_plates)]}\n",
        "test_plates = {plates[i] for i in perm[int((1. - fraction_test) * n_plates):]}\n",
        "\n",
        "print(len(train_plates), len(train_plates \u0026 validation_plates), len(train_plates \u0026 test_plates))\n",
        "print(len(validation_plates), len(validation_plates \u0026 test_plates))\n",
        "print(len(test_plates))\n",
        "\n",
        "train_subset = emb_df.plate.isin(train_plates)\n",
        "validation_subset = emb_df.plate.isin(validation_plates)\n",
        "test_subset = emb_df.plate.isin(test_plates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jw3GCQTtmc7"
      },
      "outputs": [],
      "source": [
        "# classification task: predict whether embedding came from uninfected control well\n",
        "y = (emb_df.actives == 'uninfected_control').to_numpy().astype(np.int32)\n",
        "x = (emb_df[[str(x) for x in list(range(64, 192))]].to_numpy().astype(np.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wgw_9gDato4h"
      },
      "outputs": [],
      "source": [
        "x.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VScDTNtZtrzy"
      },
      "outputs": [],
      "source": [
        "# We'll use LightGBM because it appears to work more reliably than the neural\n",
        "# nets I tried, all of which overfit pretty badly. We regularize LightGBM by\n",
        "# limiting the maximum tree depth to 2, adding L1 and L2 penalties, and\n",
        "# requiring at least 40 observations per tree node.\n",
        "\n",
        "# tuning lgbm parameters: https://neptune.ai/blog/lightgbm-parameters-guide\n",
        "\n",
        "n_estimators = 200\n",
        "\n",
        "lgbm_artifact = lightgbm.LGBMClassifier(\n",
        "    objective='binary',\n",
        "    n_estimators=n_estimators,\n",
        "    class_weight='balanced',\n",
        "    boosting='goss',\n",
        "    learning_rate=0.25,\n",
        "    # some regularization\n",
        "    max_depth=2,\n",
        "    min_data_in_leaf=40,\n",
        "    lambda_l1=0.1,\n",
        "    lambda_l2=0.5,\n",
        "    feature_fraction=0.1,\n",
        "    )\n",
        "\n",
        "lgbm_artifact.fit(\n",
        "    x[train_subset],\n",
        "    y[train_subset],\n",
        "    eval_set=[(x[validation_subset], y[validation_subset])],\n",
        "    early_stopping_rounds=n_estimators // 10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH2oO2gbocWf"
      },
      "source": [
        "We're going to treat the LightGBM output as an uncalibrated probability that an object is an artifact. Our next step is to improve the calibration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axi18xQStvIx"
      },
      "outputs": [],
      "source": [
        "# Get the LightGBM's uncalibrated P(artifact) estimate\n",
        "p_artifact = lgbm_artifact.predict_proba(x)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4VHuJ6TvEw-"
      },
      "outputs": [],
      "source": [
        "plt.hist(p_artifact, bins=50)\n",
        "plt.xlabel('P(artifact)')\n",
        "plt.ylabel('Number of patches')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh_SJdi5vMAa"
      },
      "outputs": [],
      "source": [
        "# P(artifact) for patches from uninfected control wells\n",
        "np.mean(p_artifact[emb_df.actives == 'uninfected_control'] \u003e 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyX6OdgsvMcP"
      },
      "outputs": [],
      "source": [
        "# P(artifact) for patches from infected wells\n",
        "np.mean(p_artifact[emb_df.actives != 'uninfected_control'] \u003e 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0agTqXLvQCW"
      },
      "outputs": [],
      "source": [
        "emb_df['p_artifact'] = p_artifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1e3dQ55vdjv"
      },
      "outputs": [],
      "source": [
        "emb_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd8MQM2gvV6f"
      },
      "outputs": [],
      "source": [
        "stage = np.argmax(emb_df[['stage_infer_artifact', 'stage_infer_hypnozoite', 'stage_infer_schizont']].to_numpy(), axis=-1)\n",
        "emb_df['stage'] = stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckSU4R4Zo6jo"
      },
      "source": [
        "At the beginning of this Colab we estimated the mean number of artifacts and hypnozoites in the control wells on each plate. We'll use these control well estimates to calibrate our probabilities. Specifically, we'll use the mean number of hypnozoites and artifacts in each control well on a plate to estimate the fraction of each well's objects that are artifacts. We'll then use Platt scaling to adjust the output of our uninfected well / infected well classifier to better estimate the probability that a given object is an artifact.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmanPIk6vaz-"
      },
      "outputs": [],
      "source": [
        "# Now we need to calibrate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K6QG9A14iLS"
      },
      "outputs": [],
      "source": [
        "# limit to the control wells\n",
        "emb_df_control = emb_df[\n",
        "    (emb_df['actives'].isin({'uninfected_control', 'infected_control', 'active_control'})) \u0026\n",
        "    (emb_df['stage'] != 2)\n",
        "][['batch', 'plate', 'well', 'actives', 'p_artifact', 'stage']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncH_hqD25ILr"
      },
      "outputs": [],
      "source": [
        "emb_df_control.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTbM5oWi5Vlq"
      },
      "outputs": [],
      "source": [
        "control_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biAsYAiM5F8T"
      },
      "outputs": [],
      "source": [
        "emb_df_control = emb_df_control.merge(control_df[['batch', 'plate', 'well', 'estimated_hyp_fraction']], on=['batch', 'plate', 'well'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_GaeaFm6rYh"
      },
      "outputs": [],
      "source": [
        "emb_df_control.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxaArt6byOSe"
      },
      "outputs": [],
      "source": [
        "# For each well, get the estimated hypnozoite fraction and the uncalibrated P(artifact) for non-schizonts\n",
        "\n",
        "# Implementation detail: we'll group together wells with the same number of\n",
        "# non-schizont patches so we can estimate the calibration loss more efficiently\n",
        "# below.\n",
        "\n",
        "fraction = 1.  # fraction of wells to use for calibration\n",
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "hyp_frac_by_n = {}\n",
        "proba_by_n = {}\n",
        "for well_index, ((batch, plate, well, actives), well_df) in enumerate(emb_df_control.groupby(['batch', 'plate', 'well', 'actives'])):\n",
        "  if actives not in {'uninfected_control', 'infected_control', 'active_control'}:\n",
        "    continue\n",
        "  well_df = well_df[well_df.stage != 2]  # drop schizonts\n",
        "  n = well_df.shape[0]\n",
        "  if n == 0:\n",
        "    continue\n",
        "  key, subkey = jax.random.split(key)\n",
        "  if jax.random.uniform(subkey) \u003e fraction:\n",
        "    continue\n",
        "  if not n in hyp_frac_by_n:\n",
        "    hyp_frac_by_n[n] = []\n",
        "    proba_by_n[n] = []\n",
        "  hyp_frac_by_n[n].append(well_df.estimated_hyp_fraction.to_numpy()[0])\n",
        "  proba_by_n[n].append(jnp.array(well_df.p_artifact.to_numpy()))\n",
        "\n",
        "# for each value of n, turn lists into ndarrays\n",
        "for n in hyp_frac_by_n.keys():\n",
        "  hyp_frac_by_n[n] = jnp.array(hyp_frac_by_n[n])\n",
        "  proba_by_n[n] = jnp.stack(proba_by_n[n], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NHCd0UVyf7y"
      },
      "outputs": [],
      "source": [
        "# We know for each well the approximate fraction of artifacts\n",
        "# from the calibration Colab, but we don't know which specific\n",
        "# patches are artifacts.\n",
        "#\n",
        "# We know from the classifier for each patch the uncalibrated P(artifact)\n",
        "#\n",
        "# Here we compute the log probability that we observe a particular fraction of\n",
        "# artifacts given the uncalibrated P(artifacts). The exact probability is\n",
        "# really expensive to compute, but we can approximate it: for each patch,\n",
        "# we know P(artifact). In our idealized model, each patch's artifact state is\n",
        "# an independent bernoulli random variable with mean P and variance P*(1-P).\n",
        "# We'll approximate these Bernoulli r.v.'s with normal r.v.'s with the same\n",
        "# mean and variance. The sum of these normal r.v.'s we can compute, and then\n",
        "# we can use them to estimate the log likelihood of observing the fraction of\n",
        "# artifacts we estimated from the calibration Colab.\n",
        "def get_loss_fn(hyp_frac_by_n, proba_by_n):\n",
        "  def loss_fn(params):\n",
        "    # params = (a, log(b)) where a and b are the parameters for Platt scaling\n",
        "    a, log_b = params\n",
        "    b = jnp.exp(log_b)\n",
        "    loss = 0.\n",
        "    for n in hyp_frac_by_n.keys():\n",
        "      frac_a = 1. - hyp_frac_by_n[n]  # (?,)\n",
        "      probs = proba_by_n[n]  # (?, n)\n",
        "      scaled_probs = jax.scipy.special.expit(a + b * probs)\n",
        "      mean = jnp.mean(scaled_probs, axis=-1)\n",
        "      var = jnp.mean(scaled_probs * (1. - scaled_probs), axis=-1)\n",
        "      loss += jnp.sum(jax.scipy.stats.norm.logpdf(frac_a, loc=mean, scale=jnp.sqrt(var)), axis=0)\n",
        "    return -loss\n",
        "  return loss_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms21xMs_6I7d"
      },
      "outputs": [],
      "source": [
        "# The complexity of the loss function scales with the number of different\n",
        "# well sizes. Using all sizes is quite expensive! Here we'll just grab a subset\n",
        "# of all possible sizes to speed things up.\n",
        "n_max = max(hyp_frac_by_n.keys())\n",
        "\n",
        "hyp_frac_by_n_subset = {}\n",
        "proba_by_n_subset = {}\n",
        "for n in np.arange(10, n_max, 10):\n",
        "  if n not in hyp_frac_by_n.keys():\n",
        "    continue\n",
        "  hyp_frac_by_n_subset[n] = hyp_frac_by_n[n]\n",
        "  proba_by_n_subset[n] = proba_by_n[n]\n",
        "\n",
        "loss_fn = get_loss_fn(hyp_frac_by_n_subset, proba_by_n_subset)\n",
        "\n",
        "loss_fn_jit = jax.jit(loss_fn)\n",
        "loss_fn_grad_jit = jax.jit(jax.grad(loss_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5aV5wtO6Lb8"
      },
      "outputs": [],
      "source": [
        "platt_scaling_params = (0., 0.)  # corresponds to the original unscaled probabilities\n",
        "optimizer = optax.adam(learning_rate=1.e-2)\n",
        "\n",
        "opt_state = optimizer.init(platt_scaling_params)\n",
        "\n",
        "for i in range(1000):\n",
        "  grad = loss_fn_grad_jit(platt_scaling_params)\n",
        "  updates, opt_state = optimizer.update(grad, opt_state, platt_scaling_params)\n",
        "  platt_scaling_params = optax.apply_updates(platt_scaling_params, updates)\n",
        "  if i % 100 == 0:\n",
        "    print((float(platt_scaling_params[0]), float(platt_scaling_params[1])),\n",
        "          loss_fn_jit(platt_scaling_params), flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPruXUbcs-83"
      },
      "source": [
        "We use Platt scaling to adjust the LightGBM classifier output probabilities so they are appropriate for use in an artifact / hypnozoite classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hiTlYNq6OGV"
      },
      "outputs": [],
      "source": [
        "# Get the Platt scaled artifact fraction\n",
        "p_artifact_scaled = np.array(jax.scipy.special.expit(platt_scaling_params[0] + jnp.exp(platt_scaling_params[1]) * p_artifact))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saxp8PXRtJMl"
      },
      "source": [
        "Here we show the uncalibrated probabilities (in blue) and the calibrated probabilities (in orange). Scaling makes the model more confident. This increase in confidence is likely because the original model had a harder task: it had to determine whether an object came from an uninfected or an infected well, and for the case of an artifact, it couldn't tell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DMobapC6Wmt"
      },
      "outputs": [],
      "source": [
        "# Scaling makes our model more confident!\n",
        "plt.figure(figsize=(12, 6))\n",
        "bins = np.arange(0, 1.01, 0.01)\n",
        "plt.hist(np.array(p_artifact), bins=bins, alpha=0.5, label='Unscaled')\n",
        "plt.hist(np.array(p_artifact_scaled), bins=bins, alpha=0.5, label='Scaled')\n",
        "plt.xlabel('P(artifact)')\n",
        "plt.ylabel('Number of patches')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJrCZkyN6YhG"
      },
      "outputs": [],
      "source": [
        "# Here we visualize calibration for control wells relative to the fraction of artifacts\n",
        "# predicted by our innitial model. The blue line shows that the uninfected well / infected\n",
        "# well classifier is poorly calibrated as an artifact detector. In contrast, the model\n",
        "# after Platt scaling, the orange line, is much better calibrated.\n",
        "xval = []\n",
        "yval_unscaled = []\n",
        "yval_scaled = []\n",
        "step = 1\n",
        "\n",
        "emb_df_control_sample = emb_df_control.sample(frac=0.1)\n",
        "\n",
        "est_art_frac = np.array(1. - emb_df_control_sample.estimated_hyp_fraction)\n",
        "p_art = np.array(emb_df_control_sample.p_artifact)\n",
        "p_art_scaled = np.array(scipy.special.expit(platt_scaling_params[0] + np.exp(platt_scaling_params[1]) * p_art))\n",
        "for i in range(0, 100, step):\n",
        "  subset = ((est_art_frac \u003e= i / (100 / step)) \u0026\n",
        "            (est_art_frac \u003c (i + 1) / (100 / step)))\n",
        "  if np.sum(subset):\n",
        "    xval.append(i * step / 100)\n",
        "    yval_unscaled.append(np.mean(p_art[subset]))\n",
        "    yval_scaled.append(np.mean(p_art_scaled[subset]))\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(xval, yval_unscaled, label='Before Platt scaling')\n",
        "plt.plot(xval, yval_scaled, label='After Platt scaling')\n",
        "plt.plot((0, 1), (0, 1))\n",
        "plt.xlabel('P(artifact) estimated from model')\n",
        "plt.ylabel('Classifier probability')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipvz1Qa_C8q1"
      },
      "outputs": [],
      "source": [
        "print('Uninfected controls')\n",
        "print('Mean, unscaled', np.mean(p_artifact[emb_df.actives == 'uninfected_control']))\n",
        "print('Mean, scaled', np.mean(p_artifact_scaled[emb_df.actives == 'uninfected_control']))\n",
        "\n",
        "print('Mean \u003e 0.5, unscaled', np.mean(p_artifact[emb_df.actives == 'uninfected_control'] \u003e 0.5))\n",
        "print('Mean \u003e 0.5, scaled', np.mean(p_artifact_scaled[emb_df.actives == 'uninfected_control'] \u003e 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_CJSCmGFVkH"
      },
      "outputs": [],
      "source": [
        "print('Infected wells')\n",
        "print('Mean, unscaled', np.mean(p_artifact[emb_df.actives != 'uninfected_control']))\n",
        "print('Mean, scaled', np.mean(p_artifact_scaled[emb_df.actives != 'uninfected_control']))\n",
        "\n",
        "print('Mean \u003e 0.5, unscaled', np.mean(p_artifact[emb_df.actives != 'uninfected_control'] \u003e 0.5))\n",
        "print('Mean \u003e 0.5, scaled', np.mean(p_artifact_scaled[emb_df.actives != 'uninfected_control'] \u003e 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJWcj7-7FZ3L"
      },
      "outputs": [],
      "source": [
        "# look at the extent to which different parasite stages get classified as artifacts\n",
        "plt.hist(p_artifact_scaled[stage == 0], bins=50)\n",
        "plt.title('Previous classifier artifacts')\n",
        "plt.xlabel('P(artifact)')\n",
        "plt.show()\n",
        "\n",
        "plt.hist(p_artifact_scaled[stage == 1], bins=50)\n",
        "plt.title('Hypnozoite')\n",
        "plt.xlabel('P(artifact)')\n",
        "plt.show()\n",
        "\n",
        "plt.hist(p_artifact_scaled[stage == 2], bins=50)\n",
        "plt.title('Schizont')\n",
        "plt.xlabel('P(artifact)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtE07XhdTpTj"
      },
      "outputs": [],
      "source": [
        "# save the LightGBM model and Platt scaling parameters locally, then upload to cloud\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(CLOUD_BUCKET)\n",
        "today = datetime.datetime.now().strftime('%y-%m-%d')\n",
        "\n",
        "tmpfile_lgbm = '/tmp/lgbm.txt'\n",
        "lgbm_artifact.booster_.save_model(tmpfile_lgbm)\n",
        "filename = f'{OUTPUT_PATH[1:]}/artifact-classifier-{today}.lgbm'\n",
        "blob = bucket.blob(filename)\n",
        "blob.upload_from_filename(tmpfile_lgbm)\n",
        "print(blob.public_url)\n",
        "\n",
        "tmpfile_platt = '/tmp/platt.npy'\n",
        "with open(tmpfile_platt, 'wb') as outfile:\n",
        "  np.save(outfile, np.array(platt_scaling_params))\n",
        "filename = f'{OUTPUT_PATH[1:]}/platt-scaling-{today}.npy'\n",
        "blob = bucket.blob(filename)\n",
        "blob.upload_from_filename(tmpfile_platt)\n",
        "print(blob.public_url)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "114w2fIn0XUYPgWApK0JICq3XHx07-HWb",
          "timestamp": 1680555925795
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
