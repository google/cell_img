{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UTfXiA8EAxY"
      },
      "source": [
        "# Create Embeddings for Parasites\n",
        "\n",
        "This colab shows how to run the code to process whole site images into parasite patch embeddings.\n",
        "\n",
        "Note that this pipeline runs ML models for finding parasites within images and then determining the lifecycle stage of the parasites. These models are not included in the github repo as they vary for different experimental setups (microscopes, etc). \n",
        "\n",
        "If you have a different setup that already has parasite centers and stages, you may want to use the \"Embeddings Only\" colab example, which reads in this data from a csv and only does the patch extraction and embedding creation portions of the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epd5B9ZcyvcA"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell only the FIRST time you connect to the colab kernel\n",
        "!pip install gcsfs\n",
        "!pip install keras==2.11.0\n",
        "!git clone https://github.com/google/cell_img\n",
        "!pip install -e cell_img\n",
        "!pip3 install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhTVxADx5B2A"
      },
      "outputs": [],
      "source": [
        "import gcsfs\n",
        "import fsspec\n",
        "import os\n",
        "import datetime\n",
        "import math\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "import cell_img\n",
        "from cell_img.common import io_lib\n",
        "from cell_img.common import data_utils\n",
        "from cell_img.malaria_liver.parasite_emb import counts_lib\n",
        "from cell_img.malaria_liver.parasite_emb import make_embeddings_main_lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0Pob0p6zFps"
      },
      "outputs": [],
      "source": [
        "# Not required for connection to the Google Research Bucket\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doHxPrNPsmOS"
      },
      "outputs": [],
      "source": [
        "DATA_ROOT = 'gs://path/to/your/data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTHTbIZSFihB"
      },
      "outputs": [],
      "source": [
        "GCS_PROJECT = 'your_project_name'\n",
        "GCS_BUCKET = 'your_bucket_name'\n",
        "GCS_REGION = 'your_region_name'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1dDup_rCRyo"
      },
      "outputs": [],
      "source": [
        "# Set up the paths to the ML models for finding and staging parasites\n",
        "FINDING_MODEL_PATH = 'gs://path/to/your/object/detection/model'\n",
        "STAGING_MODEL_PATH = 'gs://path/to/your/staging_model/'\n",
        "ARTIFACT_MODEL_PATH = 'gs://path/to/your/artifact_model/'\n",
        "ARTIFACT_SCALING_PATH = 'gs://path/to/your/artifact_scaling/'\n",
        "\n",
        "# constants for the pipeline\n",
        "DEFAULT_NUM_OUTPUT_SHARDS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImzQd5bXDhkH"
      },
      "source": [
        "## Set up to run pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_GQYFEjHJJE"
      },
      "outputs": [],
      "source": [
        "def get_pipeline_options(project, bucket, region, job_name):\n",
        "  \"\"\"Returns cloud dataflow pipeline options.\"\"\"\n",
        "  today_str = datetime.datetime.now().isoformat()\n",
        "  # Replace characters Cloud DataFlow hates\n",
        "  today_str = today_str.replace(':', '-').replace('.', '-').replace('T', 't')\n",
        "  job_name = job_name.replace('_', '-')\n",
        "  options = pipeline_options.PipelineOptions(flags=[\n",
        "      '--temp_location',\n",
        "      'gs://%s/tmpfiles' % bucket,\n",
        "      '--runner',\n",
        "      'DataflowRunner',\n",
        "      '--worker_machine_type',\n",
        "      'm1-ultramem-40',\n",
        "      '--max_num_workers',\n",
        "      '24',\n",
        "      '--disk_size_gb',\n",
        "      '50',\n",
        "      '--experiments',\n",
        "      'use_runner_v2',\n",
        "      '--sdk_container_image',\n",
        "      'gcr.io/%s/cell_img_imgs:v0.3' % project,\n",
        "      '--sdk_location',\n",
        "      'container',\n",
        "      '--job_name',\n",
        "      'n-%s-%s' % (job_name, today_str),\n",
        "  ])\n",
        "  options.view_as(pipeline_options.SetupOptions).save_main_session = False\n",
        "  options.view_as(pipeline_options.GoogleCloudOptions).project = project\n",
        "  options.view_as(pipeline_options.GoogleCloudOptions).region = region\n",
        "  dataflow_gcs_location = 'gs://%s/dataflow' % bucket\n",
        "  options.view_as(pipeline_options.GoogleCloudOptions\n",
        "                 ).staging_location = '%s/staging' % dataflow_gcs_location\n",
        "  options.view_as(pipeline_options.GoogleCloudOptions\n",
        "                 ).temp_location = '%s/temp' % dataflow_gcs_location\n",
        "  return options\n",
        "\n",
        "\n",
        "def run_pipeline(image_csv_path, metadata_csv_path, output_dir,\n",
        "                 whole_image_size, num_output_shards=10, options=None):\n",
        "\n",
        "  # For most of the options, we hardcode the correct values for our\n",
        "  # pipeline. Most of these options shouldn't ever change.\n",
        "  pipeline_result = make_embeddings_main_lib.run_embeddings_pipeline(\n",
        "    image_csv=image_csv_path,\n",
        "    metadata_csv=metadata_csv_path,\n",
        "    raw_channel_order=['w1', 'w2', 'w3'],\n",
        "    channel_order=['DAPI', 'PVM', 'HSP70'],\n",
        "    whole_image_size=whole_image_size,\n",
        "    log_brightness_min=[-7., -7., -7.],\n",
        "    log_brightness_max=[0., 0., 0.],\n",
        "    output_dir=output_dir,\n",
        "    model_path=STAGING_MODEL_PATH,\n",
        "    min_confidence=0.6,\n",
        "    pixels_per_side=64,\n",
        "    count_csv_dir='count_csvs',\n",
        "    crop_size=32,\n",
        "    stain_indices=[2, 1],\n",
        "    do_rotate=True,\n",
        "    do_center=True,\n",
        "    batch_size=128,\n",
        "    embedding_model_path='https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2',\n",
        "    embedding_model_output_size=64,\n",
        "    embedding_model_output_seed=2342343,\n",
        "    staging_model_path=STAGING_MODEL_PATH,\n",
        "    staging_channel_order=[0, 1, 2],\n",
        "    artifact_model_path=ARTIFACT_MODEL_PATH,\n",
        "    artifact_scaling_path=ARTIFACT_SCALING_PATH,\n",
        "    num_output_shards=num_output_shards,\n",
        "    options=options)\n",
        "\n",
        "  if hasattr(pipeline_result, '_job'):\n",
        "    # It was launched on cloud dataflow. Print the URL for the job.\n",
        "    url = ('https://console.cloud.google.com/dataflow/jobs/%s/%s?project=%s' %\n",
        "          (pipeline_result._job.location,\n",
        "          pipeline_result._job.id,\n",
        "          pipeline_result._job.projectId))\n",
        "    print(url)\n",
        "  return pipeline_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2siXz6PX3hpe"
      },
      "source": [
        "## Local test with local images\n",
        "\n",
        "This runs a local instance on test data, images that are just noise.\n",
        "\n",
        "Note that you can look at the files on your local colab kernel using the directory button on the left side of the screen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh276st44uTt"
      },
      "outputs": [],
      "source": [
        "from PIL import Image as PilImage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyRxUlwbVmfw"
      },
      "outputs": [],
      "source": [
        "%%writefile example_image.csv\n",
        "plate_uid,well,channel,site,image_path\n",
        "1604,F09,w1,14,im1.tif\n",
        "1604,F09,w2,14,im2.tif\n",
        "1604,F09,w3,14,im3.tif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK1dkMt2309l"
      },
      "outputs": [],
      "source": [
        "%%writefile example_metadata.csv\n",
        "plate_uid,well,blinded_code,stain_map,batch\n",
        "1604,F09,XE-50-HB11,0,Pc22-022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xfTqfHa4-Vr"
      },
      "outputs": [],
      "source": [
        "EXAMPLE_IMAGE_SIZE = [100, 200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB7n8_NpWNYo"
      },
      "outputs": [],
      "source": [
        "example_image_metadata_df = pd.read_csv('example_image.csv')\n",
        "\n",
        "np.random.seed(12345) # For determinism.\n",
        "\n",
        "def write_test_image(image_path):\n",
        "  array = np.random.randint(0, 65535, size=EXAMPLE_IMAGE_SIZE, dtype='uint16')\n",
        "  pil_image = PilImage.fromarray(array)\n",
        "  with open(image_path, 'wb') as f:\n",
        "    pil_image.save(f, 'tiff')\n",
        "  print('Wrote to %s' % image_path)\n",
        "\n",
        "_ = example_image_metadata_df['image_path'].map(write_test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqKAXZm8qCX7"
      },
      "outputs": [],
      "source": [
        "run_pipeline('example_image.csv', 'example_metadata.csv', 'example_out_dir',\n",
        "             num_output_shards=1, whole_image_size=EXAMPLE_IMAGE_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmdNVtbIcsc8"
      },
      "outputs": [],
      "source": [
        "pd.read_parquet('example_out_dir/patches.parquet-00000-of-00001')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nArvG9QN1iU"
      },
      "source": [
        "## Local Test with remote images\n",
        "\n",
        "Similar to the local test above, but using real images on the cloud bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mut_b0B8N4T5"
      },
      "outputs": [],
      "source": [
        "%%writefile real_example_image.csv\n",
        "plate_uid,well,channel,site,image_path\n",
        "1463,K05,w1,13,gs://path/to/your/data/image_data/1463/your_image_for_K05_s13_w1.tif\n",
        "1463,K05,w2,13,gs://path/to/your/data/image_data/1463/your_image_for_K05_s13_w2.tif\n",
        "1463,K05,w3,13,gs://path/to/your/data/image_data/1463/your_image_for_K05_s13_w3.tif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2RRkKMiN4T6"
      },
      "outputs": [],
      "source": [
        "%%writefile real_example_metadata.csv\n",
        "plate_uid,well,blinded_code,stain_map,batch\n",
        "1463,K05,RA-17-GT96,0,Pc22-018"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQFpkISkWbEc"
      },
      "outputs": [],
      "source": [
        "# The pipeline is very memory intensive on real files, do\n",
        "# our best to clear up any unused memory before running.\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T8uklEYN4T7"
      },
      "outputs": [],
      "source": [
        "run_pipeline('real_example_image.csv', 'real_example_metadata.csv', 'real_example_out_dir',\n",
        "             num_output_shards=1, whole_image_size=[2048,2048])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2z_CeBQR2_7"
      },
      "outputs": [],
      "source": [
        "pd.read_parquet('real_example_out_dir/patches.parquet-00000-of-00001')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd33-I-gR7Hb"
      },
      "outputs": [],
      "source": [
        "pd.read_parquet('real_example_out_dir/hypnozoite_patches.parquet-00000-of-00001')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n01kZQW0DEoB"
      },
      "source": [
        "## Run embedding creation job on Cloud Dataflow\n",
        "\n",
        "Running on Google Cloud DataFlow requires connecting to your own Google Cloud instance. The code below will not run with the default setup that is connected to the Google Research Cloud Bucket, but we include it as an example as it is required to complete larger jobs efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQYHvKgdkDlq"
      },
      "source": [
        "Note on how this pipeline is set up: we're using a docker container to pre-install the python dependencies on cloud workers. This is how it's done - \n",
        "\n",
        "1. Go to cloud shell from Google Cloud Console\n",
        "\n",
        "2. Make a file called \"Dockerfile\", with contents\n",
        "```\n",
        "FROM apache/beam_python3.9_sdk:2.46.0\n",
        "RUN pip install gcsfs\n",
        "RUN pip install keras==2.11.0\n",
        "RUN git clone https://github.com/google/cell_img\n",
        "RUN pip install -e cell_img\n",
        "RUN pip3 install --upgrade tensorflow\n",
        "RUN pip install --upgrade pip\n",
        "RUN pip install --upgrade \"jax[cpu]\"\n",
        "RUN pip install lightgbm\n",
        "RUN pip install scikit-image\n",
        "RUN pip install tensorflow-hub\n",
        "RUN gcloud config set project YOUR_PROJECT_NAME\n",
        "```\n",
        "(One way this docker container may fail is if the version of python colab kernels use gets updated to python 3.10 or higher. The list of available beam docker images are [here](https://hub.docker.com/search?q=apache%2Fbeam\u0026type=image). Update the first line after FROM to the docker image with a matching python version found in the link.)\n",
        "\n",
        "3. Push the docker image by typing this in the shell; make sure to change TAG\n",
        "```\n",
        "export TAG={WRITE THE NEW TAG! IT WAS ORIGINALLY v0.3}\n",
        "export PROJECT=YOUR_PROJECT_NAME\n",
        "export REPO=cell_img_imgs\n",
        "export IMAGE_URI=gcr.io/$PROJECT/$REPO:$TAG\n",
        "docker build . --tag $IMAGE_URI\n",
        "docker push $IMAGE_URI\n",
        "```\n",
        "\n",
        "4. In get_pipeline_options, update the ```--sdk_container_image``` flag with the new ```$IMAGE_URI```. If you don't remember what you used, type ```echo $IMAGE_URI``` to cloud shell again and copy-paste the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7WADfbPeRhx"
      },
      "outputs": [],
      "source": [
        "import apache_beam as beam\n",
        "from apache_beam.options import pipeline_options\n",
        "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
        "from apache_beam.runners import DataflowRunner\n",
        "\n",
        "from cell_img.malaria_liver.parasite_emb import make_embeddings_main_lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvBaU5_9gJr2"
      },
      "outputs": [],
      "source": [
        "options = get_pipeline_options(GCS_PROJECT, GCS_BUCKET, GCS_REGION, job_name)\n",
        "pipeline_result = run_pipeline(\n",
        "      image_csv_path=image_csv_path,\n",
        "      metadata_csv_path=metadata_csv_path,\n",
        "      whole_image_size=image_size_to_use,\n",
        "      output_dir=output_dir,\n",
        "      num_output_shards=DEFAULT_NUM_OUTPUT_SHARDS,\n",
        "      options=options,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1_cT7d44tHFAFn2S631nOxOwRBfbMCD_E",
          "timestamp": 1680294547269
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
